<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xml:lang="nl-nl" lang="nl-nl">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta name="copyright" content="(C) Copyright 2005"/>
<meta name="DC.rights.owner" content="(C) Copyright 2005"/>
<meta name="DC.Type" content="kpe-concept"/>
<meta name="DC.Title" content="Single-Factor Regression Models"/>
<meta name="DC.Format" content="XHTML"/>
<meta name="DC.Identifier" content="caia_topic_00001_80"/>
<meta name="DC.Language" content="nl-NL"/>
<link rel="stylesheet" type="text/css" href="..%5Ccommonltr.css"/>
<title>Single-Factor Regression Models</title>
</head>
<body id="caia_topic_00001_80">

<h1 class="title topictitle1"><strong class="ph b">Single-Factor Regression Models </strong></h1>
<div class="body conbody kpe-conceptBody"><p class="p"><strong class="ph b">LO 9.1: Demonstrate knowledge of single-factor regression models.</strong></p>
<p class="p"><strong class="ph b">For example:</strong></p>
<ul class="ul"><li class="li"><strong class="ph b">•Explain the use of ordinary least squares to estimate regression parameters.</strong></li>
<li class="li"><strong class="ph b">Describe the problem outliers pose to regression analysis.</strong></li>
<li class="li"><strong class="ph b">Describe the problem autocorrelation poses to regression analysis.</strong></li>
<li class="li"><strong class="ph b">Describe the problem heteroskedasticity poses to regression analysis.</strong></li>
<li class="li"><strong class="ph b">Interpret a regression’s goodness of fit.</strong></li>
<li class="li"><strong class="ph b">Evaluate the statistical significance of regression parameter</strong> <strong class="ph b">estimates.</strong></li>
<li class="li"><strong class="ph b">Calculate the </strong><span class="ph">t</span><strong class="ph b">-statistic.</strong></li>
</ul>
<p class="p">A <strong class="ph b">regression </strong>is a statistical method that describes the relationship between a <strong class="ph b">dependent variable</strong> and one or more <strong class="ph b">independent variables</strong>. A <strong class="ph b">linear regression </strong>describes a linear relationship for the dependent variable. Independent variables also are known as <em class="ph i">explanatory variables</em>. </p>
<p class="p">A <strong class="ph b">simple linear regression</strong> is a statistical method that models a linear relationship between a dependent variable and a single independent variable. Therefore, a simple linear regression fits a line to a scatter of paired observations for the dependent and independent variable. The equation for a simple linear CAPM-based regression is:</p>
<p class="p">R<sub class="ph sub">it</sub> – R<sub class="ph sub">f </sub>= a<sub class="ph sub">i</sub> + B<sub class="ph sub">i</sub>(R<sub class="ph sub">mt</sub> – R<sub class="ph sub">f</sub>) + e<sub class="ph sub">it</sub></p>
<p class="p">where: </p>
<p class="p">R<sub class="ph sub">i</sub> = stock return for asset <em class="ph i">i </em>in period <em class="ph i">t</em></p>
<p class="p">R<sub class="ph sub">f</sub> = risk-free rate</p>
<p class="p">R<sub class="ph sub">mt</sub> = market portfolio return for period <em class="ph i">t</em></p>
<p class="p">a<sub class="ph sub">i</sub> = regression intercept estimate for asset <em class="ph i">i</em></p>
<p class="p">B<sub class="ph sub">i</sub> = regression slope estimate for asset <em class="ph i">i</em></p>
<p class="p">e<sub class="ph sub">it</sub> = regression residuals for asset <em class="ph i">i</em> in period <em class="ph i">t</em></p>
<p class="p">The following figure illustrates a CAPM-based regression.</p>
<div class="fig fignone"><span class="figcap">Figuur 1. <strong class="ph b">Figure 1: CAPM-Based Regression</strong> </span></div>
<p class="p">The regression equation for the figure above is:</p>
<p class="p">regression estimate of asset excess return = 0.02 + 1.50(R<sub class="ph sub">mt</sub> – R<sub class="ph sub">f</sub>)</p>
<p class="p">The estimated <strong class="ph b">slope coefficient </strong>of the simple linear regression equals the expected change in the dependent variable for every 1-unit change in the independent variable. In a CAPM-based regression, the slope coefficient equals the asset’s <strong class="ph b">beta</strong>, which is the sensitivity of the asset’s returns to changes in the market portfolio returns. </p>
<p class="p">In the previous figure, the slope coefficient equals 1.5, indicating that, on average, the asset’s excess return changes by 1.5 percentage points for every 1 percentage point change in the market portfolio excess return. </p>
<p class="p">The <strong class="ph b">intercept </strong>of the CAPM-based regression equals the incremental performance of the asset relative to the CAPM benchmark return, and is called the asset’s <strong class="ph b">alpha</strong>. In Figure 1, the 2% alpha estimate indicates that, on average, the fund’s return exceeded the ex post CAPM expected or “required” return by 200 basis points. When applied to portfolios, the alpha measures the return attributable to skill or luck of the portfolio manager. </p>
<p class="p">The <strong class="ph b">residuals </strong>in the regression formula reflect the deviation of idiosyncratic realized returns from the mean idiosyncratic return (i.e., the estimate of the error term).</p>
<p class="p"><em class="ph i">Professor’s</em><span class="ph"> </span><em class="ph i">Note: Application of the CAPM-based regression for benchmarking was discussed in Topic 2.7.</em></p>
<div class="section"><h2 class="title sectiontitle"><strong class="ph b">Ordinary Least Squares Method </strong></h2><p class="p"><strong class="ph b">Ordinary least squares (OLS) </strong>is a statistical method that derives estimates that minimize the sum of squared residuals. The residuals are the differences between the dependent variable and the regression estimate of the dependent variable (e.g., the vertical distances between the scatter points and the regression line). Therefore, in the simple linear regression, the regression line best fits the scatter of points. The OLS method is most likely to generate accurate, unbiased estimates if the regression residuals are normally distributed, uncorrelated, and homoskedastic. Each assumption and violations of the assumption are discussed below.</p>
<p class="p"><em class="ph i">Normal distribution.</em> The normality assumption often is violated when data contain <strong class="ph b">outliers</strong>, which are data points with extreme values. Alternative investment returns often are leptokurtic, implying a greater number of outliers relative to the normal distribution. Outliers have disproportionate effects during the residual squaring process of the OLS method. To identify outliers in a dataset, we can plot the residuals versus the independent variable, or plot the residuals versus time. An outlier can be removed from the regression if the analyst determines that the event underlying an outlier is unlikely to repeat.</p>
<p class="p"><em class="ph i">Uncorrelated. </em>The OLS method assumes that regression residuals are uncorrelated with their lagged values. The consequence of serial correlation (also known as autocorrelation) is that the standard errors and <em class="ph i">t</em>-statistics estimated by the regression will be invalid. Tests of first order autocorrelation can be performed with the Durbin-Watson test statistic (discussed at length in Topic 2.4). </p>
<p class="p"><em class="ph i">Homoskedasticity. </em>The OLS method assumes that the variance of the residuals is constant (i.e., the residuals are assumed to be homoskedastic). <strong class="ph b">Heteroskedasticity</strong> refers to a violation of the constant error variance assumption. <em class="ph i">Conditional heteroskedasticity</em> is heteroskedasticity that is related to the level of (i.e., conditional on) the independent variables. For example, conditional heteroskedasticity exists if the variance of the residuals changes as the value of the independent variable changes, as shown in Figure 2. Notice in this figure that the residual variance associated with the larger values of the independent variable, <em class="ph i">X</em>, is larger than the residual variance associated with the smaller values of <em class="ph i">X</em>. The consequence of conditional heteroskedasticity is that the standard errors and <em class="ph i">t</em>-statistics estimated by the regression will be invalid.</p>
<div class="fig fignone"><span class="figcap">Figuur 2. <strong class="ph b">Figure 2: Conditional Heteroskedasticity </strong></span></div>
<div class="note sample note"><span class="notetitle">sample:</span> <p class="p label"><strong class="ph b">Example: Detecting heteroskedasticity with a residual             plot</strong></p>
<p class="p">You have been studying the monthly returns of a mutual fund over the past five years, hoping to draw conclusions about the fund’s average performance. You calculate the mean return, the standard deviation, and the portfolio’s beta by regressing the fund’s returns on S&amp;P 500 Index returns (the independent variable). The standard deviation of returns and the fund’s beta do not seem to fit the firm’s stated risk profile. For your analysis, you have prepared a scatter plot of the residuals (actual return – predicted return) for the regression using five years of returns, as shown in the following figure. <strong class="ph b">Determine</strong> whether the residual plot indicates that there may be a problem with the data.</p>
<p class="p"><strong class="ph b">Residual Plot</strong></p>
<p class="p"><strong class="ph b">Answer</strong>:</p>
<p class="p">The residual plot in the previous figure indicates the presence of conditional heteroskedasticity. Notice how the variation in the regression residuals increases as the independent variable increases. This indicates that the variance of the fund’s returns about the mean is related to the level of the independent variable.</p>
</div>
<p class="p"><strong class="ph b">Interpreting Goodness of Fit</strong></p>
<p class="p">The <strong class="ph b">goodness of fit statistic</strong>, or <span class="ph">R</span><strong class="ph b">-squared </strong>value of the regression, is a measure of its overall explanatory power. It is equal to the percent of the variation in the dependent variable explained by the independent variables. The <em class="ph i">R</em>-squared ranges from 0 to + 1. For instance, when applied to a CAPM-based regression, an <em class="ph i">R</em>-squared of 0.80 indicates that 80% of the variation in the asset’s excess returns is explained by the market’s excess returns. In a simple linear regression, the <em class="ph i">R</em>-squared equals the square of the correlation between the dependent and independent variable. The percent of the asset’s variation attributable to idiosyncratic risk equals 1 minus the <em class="ph i">R</em>-squared. </p>
<p class="p"><strong class="ph b">T-tests and Statistical Significance </strong></p>
<p class="p">Estimates of the individual parameters derived from a regression can also be tested for statistical significance using a <em class="ph i">t</em>-test. The <span class="ph">t</span><strong class="ph b">-test</strong> is a widely used hypothesis test that employs a test statistic that is distributed according to a <em class="ph i">t</em>-distribution. To conduct a <em class="ph i">t</em>-test, the <strong class="ph b">t-statistic </strong>(which equals the parameter estimate divided by its standard error) is compared to a critical <em class="ph i">t</em>-value at the desired level of significance with the appropriate degrees of freedom. The result indicates if the estimate is significantly different from zero. In large samples, using a 5% level of significance, we can conclude that the intercept or slope estimate is statistically significant if its <em class="ph i">t</em>-statistic exceeds 1.96 in absolute value, implying that the estimate is more than 1.96 standard errors removed from zero.</p>
<div class="note sample note"><span class="notetitle">sample:</span> <p class="p label"><strong class="ph b">Example: Interpreting regression results</strong></p>
<p class="p">Consider a large sample CAPM-based regression with the following results:</p>
</div>

<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" class="table" frame="border" border="1" rules="all"><tbody class="tbody"><tr class="row"><td class="entry" valign="top" width="28.882602545968883%"><div class="note sample note"><span class="notetitle">sample:</span> </div>
</td>
<td class="entry" valign="top" width="33.74823196605375%"><div class="note sample note"><span class="notetitle">sample:</span> <p class="p">Estimate</p>
</div>
</td>
<td class="entry" valign="top" width="37.36916548797737%"><div class="note sample note"><span class="notetitle">sample:</span> <p class="p">Standard Error</p>
</div>
</td>
</tr>
<tr class="row"><td class="entry" valign="top" width="28.882602545968883%"><div class="note sample note"><span class="notetitle">sample:</span> <p class="p">Intercept</p>
</div>
</td>
<td class="entry" valign="top" width="33.74823196605375%"><div class="note sample note"><span class="notetitle">sample:</span> <p class="p">0.02</p>
</div>
</td>
<td class="entry" valign="top" width="37.36916548797737%"><div class="note sample note"><span class="notetitle">sample:</span> <p class="p">0.10</p>
</div>
</td>
</tr>
<tr class="row"><td class="entry" valign="top" width="28.882602545968883%"><div class="note sample note"><span class="notetitle">sample:</span> <p class="p">Slope</p>
</div>
</td>
<td class="entry" valign="top" width="33.74823196605375%"><div class="note sample note"><span class="notetitle">sample:</span> <p class="p">1.25</p>
</div>
</td>
<td class="entry" valign="top" width="37.36916548797737%"><div class="note sample note"><span class="notetitle">sample:</span> <p class="p">0.50</p>
</div>
</td>
</tr>
</tbody>
</table>
</div>
<div class="note sample note"><span class="notetitle">sample:</span> <p class="p"><strong class="ph b">Determine</strong> the statistical significance of the intercept and the slope, using a 5% level of significance.</p>
<p class="p"><strong class="ph b">Answer</strong>:</p>
<p class="p">The <em class="ph i">t</em>-statistic for the intercept equals 0.02/0.10 = 0.20, and the <em class="ph i">t</em>-statistic for the slope equals 1.25/0.50 = 2.50. The slope estimate is statistically significant because its <em class="ph i">t</em>-statistic exceeds the critical value of 1.96. The intercept estimate is not statistically significant</p>
</div>
</div>
</div>

</body>
</html>