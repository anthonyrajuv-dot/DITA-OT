<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xml:lang="nl-nl" lang="nl-nl">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta name="copyright" content="(C) Copyright 2005"/>
<meta name="DC.rights.owner" content="(C) Copyright 2005"/>
<meta name="DC.Type" content="kpe-concept"/>
<meta name="DC.Title" content="Hypothesis Testing Steps"/>
<meta name="DC.Format" content="XHTML"/>
<meta name="DC.Identifier" content="caia_topic_00001_75"/>
<meta name="DC.Language" content="nl-NL"/>
<link rel="stylesheet" type="text/css" href="..%5Ccommonltr.css"/>
<title>Hypothesis Testing Steps</title>
</head>
<body id="caia_topic_00001_75">

<h1 class="title topictitle1">Hypothesis Testing Steps</h1>
<div class="body conbody kpe-conceptBody"><p class="p">LO 8.7: Demonstrate knowledge of statistical methods for locating alpha.</p>
<p class="p">For example:</p>
<ul class="ul"><li class="li">•Identify the four steps of hypothesis testing (i.e., state the hypothesis, formulate an analysis plan, analyze sample data, and interpret results).</li>
<li class="li">•Recognize the components of hypothesis statements (i.e., null hypothesis and alternative hypothesis).</li>
<li class="li">•Describe the process of designing hypothesis tests.</li>
<li class="li">•Describe the process of creating test statistics for use in analyzing sample data.</li>
<li class="li">Explain the decision-making process for rejecting or failing to reject the null hypothesis.</li>
<li class="li">Recognize the four common problems with using inferential statistics (i.e., misinterpretation of high p-values, failure to distinguish between statistical significance and economic significance, violation of distributional assumptions, and misinterpretation of level of confidence).</li>
<li class="li">Define and discuss type I and type II errors in hypothesis testing.</li>
</ul>
<p class="p"><strong class="ph b">Hypotheses</strong> are propositions that underlie an analysis. Hypothesis testing procedures, based on sample statistics and probability theory, are used to determine whether a hypothesis is a reasonable statement and should not be rejected or if it is an unreasonable statement and should be rejected. </p>
<p class="p">The four steps in hypothesis testing are:</p>
<p class="p"><span class="ph">Step 1: </span>State the hypothesis.</p>
<p class="p"><span class="ph">Step 2: </span>Form an analysis plan (design the statistical test).</p>
<p class="p"><span class="ph">Step 3: </span>Analyze the sample data (calculate the test statistic).</p>
<p class="p"><span class="ph">Step 4: </span>Interpret the results (reject or fail to reject the null hypothesis).</p>
<div class="section"><h2 class="title sectiontitle">Step 1: State the Hypothesis</h2><p class="p">The most important step in hypothesis testing is stating the components of a hypothesis statement. The first component is the <strong class="ph b">null hypothesis</strong>, which usually is the statement that the analyst attempts to reject. The second component is the <strong class="ph b">alternative hypothesis</strong>, which is the opposite claim of the null hypothesis and represents the behavior that exists if the null hypothesis is false. </p>
<p class="p">The null and the alternative hypotheses are usually mutually exclusive and are complements to each other. Therefore, rejecting the null hypothesis is interpreted as supporting the alternative hypothesis. </p>
</div>
<div class="section"><h2 class="title sectiontitle">Step 2: Form an Analysis Plan</h2><p class="p">After the null hypothesis is established, the next step is to design a method to test the hypothesis. The null hypothesis is examined using a <strong class="ph b">test statistic</strong>, which is a function of the observed values of the random variables of interest, based on distributional assumptions for the data. Large values of the test statistic indicate the sampled data are far from expected, providing evidence against the null hypothesis and in favor of the alternative hypothesis.</p>
<p class="p">To conduct the test, a <strong class="ph b">significance level</strong> must be established. The significance level denotes the probability that a significant result may be due to random chance. Often significance levels are set at 1%, 5%, or 10%. The <strong class="ph b">confidence level</strong> equals 100% minus the significance level. A <strong class="ph b">confidence interval </strong>is a range of values around the expected outcome within which we expect the actual outcome to be some specified percentage of the time. For example, a 99% confidence level for an investment’s beta may have a confidence interval of 0.5 to 1.3.</p>
</div>
<div class="section"><h2 class="title sectiontitle">Step 3: Analyze the Sample Data</h2><p class="p">The next step is to derive the value of the test statistic, which is used to test the null hypothesis. The test statistic is calculated from the data and is compared against the predetermined critical value to make a reject or fail to reject decision regarding the null hypothesis. </p>
<p class="p">The test statistic is often standardized, such as: </p>
<p class="p">The numerator is the difference between the calculated sample statistic and the value stated in the null hypothesis. The denominator is the standard error of the statistic, defined as the standard deviation of the sample statistic and is a measure of the precision of the statistic. </p>
<p class="p">The test statistic quantifies how far the estimated value is from the hypothesized value, in standard deviation units. The standardization process of the test statistic allows the test statistic to have a zero mean and unit standard deviation under the null hypothesis. </p>
<p class="p">Based on the distributional assumptions of the test statistic, the<strong class="ph b"> </strong><span class="ph">p</span><strong class="ph b">-value</strong> can be determined, which also can be used to test the null hypothesis. For example, a <em class="ph i">p-</em>value of 0.022 implies that, assuming the null hypothesis is true, there is a 2.2% chance of finding a value as extreme as the one derived from the sample.</p>
</div>
<div class="section"><h2 class="title sectiontitle">Step 4: Interpret the Results</h2><p class="p">The decision rule is to reject the null hypothesis if the calculated test statistic exceeds its critical value or if the <em class="ph i">p-</em>value is less than the significance level. Identical decisions will be reached using the test statistic or the <em class="ph i">p-</em>value decision rule. As the test statistic rises, the <em class="ph i">p-</em>value falls. If the test statistic exceeds its critical value, then the <em class="ph i">p-</em>value will be less than the significance level. </p>
</div>
<div class="section"><h2 class="title sectiontitle">Failing to Reject the Null Hypothesis vs. Accepting the Null Hypothesis</h2><p class="p">It is important to understand the difference between <em class="ph i">failing to reject</em> and <em class="ph i">accepting</em> the null hypothesis. Consider a person charged with a crime. </p>
<p class="p">H<sub class="ph sub">0</sub>: The accused person is innocent of the crime.</p>
<p class="p">H<sub class="ph sub">1</sub>: The accused person is guilty of the crime. </p>
<p class="p">The test of the hypotheses is the trial of the accused, and the data is the evidence presented by both sides. After the evidence is presented, the jury evaluates the data and delivers the decision. If the evidence is substantial to show that the accused committed the crime, the jury rejects the null hypothesis that the accused is innocent in favor of the alternative hypothesis that the accused is guilty. </p>
<p class="p">However, if the evidence is not substantial, the jury’s verdict is <em class="ph i">not guilty</em>, which is the equivalent of failing to reject the null hypothesis. The proper conclusion is that the evidence is insufficient to show that the null hypothesis is false (i.e., evidence was insufficient to reject the hypothesis of innocence). </p>
<p class="p">Lack of sufficient evidence does not imply that the null hypothesis is true. Therefore, we cannot claim that we accept the null hypothesis. Instead, we state that we fail to reject the null hypothesis. Note that the jury’s verdict is not <em class="ph i">innocent</em>, which is the equivalent of accepting the null hypothesis, but instead is <em class="ph i">not guilty</em>, which is the equivalent of not rejecting the null hypothesis. </p>
<p class="p">In summary, statistical tests are designed to disprove rather than to prove the null hypothesis statement.</p>
</div>
<div class="section"><h2 class="title sectiontitle">Four Common Problems Using Inferential Statistics </h2><p class="p">A result is statistically significant if it is unlikely to have occurred merely by chance. Common errors in the interpretation of statistical significance relate to:</p>
<ul class="ul"><li class="li">Strength of relationships.</li>
<li class="li">Economic significance.</li>
<li class="li">Distribution assumptions.</li>
<li class="li">Level of confidence.</li>
</ul>
</div>
<div class="section"><h2 class="title sectiontitle">Strength of Relationships</h2><p class="p">Outcomes with lower <em class="ph i">p-</em>values (or higher test<em class="ph i"> </em>statistics) often are mistaken to indicate stronger relationships. For instance, an outcome with a <em class="ph i">p-</em>value less than 0.01 is misinterpreted to have a stronger relationship than the outcome with a <em class="ph i">p-</em>value less than 0.05. Both <em class="ph i">p-</em>values are less than 0.05, which supports the proposition that relationships exist (using a 5% level of significance), but the <em class="ph i">p-</em>values should not be used to indicate the strength of relationship.</p>
<p class="p">For example, consider a hypothesis test performed on a portfolio beta. Assume two tests are run on the same portfolio but with different sample sizes. In the first test, the portfolio beta equals 0.50 with a <em class="ph i">p-</em>value of 0.04. In the second test, a larger sample for the same portfolio is examined, in which the beta equals 0.50, with a <em class="ph i">p-</em>value of 0.01. Using a 95% confidence level, the beta is significantly different from zero in both tests, indicating that a relationship exists between the returns of the portfolio and the market. The <em class="ph i">p-</em>value is smaller for the second test because the number of observations is greater, not because the relationship is stronger.</p>
<p class="p"><em class="ph i">Professor’s</em><span class="ph"> </span><em class="ph i">Note: The standard error usually becomes smaller as the sample size becomes larger, causing </em>t<em class="ph i">-statistics to rise and </em>p<em class="ph i">-values to fall.</em></p>
</div>
<div class="section"><h2 class="title sectiontitle">Economic Significance</h2><p class="p">A common error is to mistake statistical significance for <strong class="ph b">economic significance</strong>. The test statistic might exceed its critical value because the standard error (denominator of the test statistic) is small, not because the estimate (numerator of the test statistic) is large. Economic significance describes the extent to which a variable has a meaningful impact. </p>
<p class="p">Similarly, sometimes variables that establish no statistical significance could have a substantial economic significance. After considering the absolute size of the parameter and the dispersion in the related explanatory variable, the analyst might determine that the relationship, if true, would have a material impact on the model. </p>
</div>
<div class="section"><h2 class="title sectiontitle">Distribution Assumptions</h2><p class="p">A <em class="ph i">p</em>-value is calculated assuming the data follow a particular kind of distribution (e.g., a normal distribution). When interpreting a <em class="ph i">p</em>-value, an analyst should confirm whether the data are indeed distributed in the assumed way. The <em class="ph i">p</em>-value is not meaningful if the data violate the distributional assumption.</p>
</div>
<div class="section"><h2 class="title sectiontitle">Level of Confidence</h2><p class="p">Another common error is to confuse the confidence level for the probability that a relationship exists. For example, assume we reject the null hypothesis that a venture capital (VC) firm’s profits, <span class="ph"></span>, equal zero, using a 99% confidence level. The null hypothesis and alternative hypothesis are:</p>
<p class="p">H<sub class="ph sub">0</sub>: <span class="ph"></span> = 0</p>
<p class="p">H<sub class="ph sub">1</sub>: <span class="ph"></span> <span class="ph"></span> 0</p>
<p class="p">What is the probability that the VC’s profits do not equal zero, given that the null hypothesis (of zero profits) is rejected for the firm? The common mistake is to assume the answer equals the confidence level (99%), but that is a wrong interpretation of the confidence level. For instance, consider a market in which the probability that VC profits equal zero is 0.9999, and the probability that VC profits do not equal zero equals 0.0001. Assume the test is performed using a 99% confidence level (or a 1% significance level). Therefore, there is a 1% chance that the null is rejected for VC firms that truly earn zero profits. That is, there is approximately a 1% probability that the VC’s profits are nonzero, given that the null hypothesis is rejected. Stated differently, there is a 1% probability that the alternative hypothesis is correct, given that the null hypothesis is rejected. The main point is that the confidence level does not equal the probability that a relationship exists (i.e., that the alternative hypothesis is true), and the difference can be substantial.</p>
<p class="p"><strong class="ph b">Type I and Type II Errors</strong></p>
<p class="p">The two major types of errors made in hypothesis testing are type I and type II errors. </p>
<p class="p">A <strong class="ph b">type I error</strong> occurs when rejecting a true null hypothesis. For example, consider a null hypothesis that an independent variable has no effect on the dependent variable. A type I error is made when the analyst attributes the results to the effect of an independent variable, when, in fact, the independent variable has no effect. The probability of a type I error is usually denoted by <span class="ph"></span> and should not be confused with the investment alpha. The term <span class="ph"></span> is the significance level, and it is selected by the analyst. The 1 – <span class="ph"></span> is called the confidence level, or specificity of the test.</p>
<p class="p">A <strong class="ph b">type II error</strong> occurs when failing to reject an untrue null hypothesis. For example, a type II error is made when the analyst concludes that an independent variable has no effect when in fact it does have an effect. The probability of a type II error is usually denoted by <span class="ph"></span> and should not be confused with the symbol used to denote systematic risk. The statistical power of the test is denoted by <em class="ph i">1 – </em><span class="ph"></span>. </p>
<p class="p">The probabilities of both type I and type II errors can be reduced by increasing the sample size. </p>
<div class="fig fignone"><span class="figcap">Figuur 1. Figure 3: Type I and Type II Errors in Hypothesis Testing</span></div>

<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" class="table" frame="border" border="1" rules="all"><tbody class="tbody"><tr class="row"><td class="entry" valign="top" width="30.00615195324515%"> </td>
<td class="entry" valign="top" width="35.00461396493387%"><p class="p"><em class="ph i">True State</em></p>
</td>
</tr>
<tr class="row"><td class="entry" valign="top" width="30.00615195324515%"><p class="p"><em class="ph i">Decision</em></p>
</td>
<td class="entry" valign="top" width="35.00461396493387%"><p class="p"><em class="ph i">Null hypothesis is true</em></p>
</td>
<td class="entry" valign="top" width="34.98923408182098%"><p class="p"><em class="ph i">Null hypothesis is false</em></p>
</td>
</tr>
<tr class="row"><td class="entry" valign="top" width="30.00615195324515%"><p class="p">Reject the null</p>
</td>
<td class="entry" valign="top" width="35.00461396493387%"><p class="p">Type I error</p>
</td>
<td class="entry" valign="top" width="34.98923408182098%"><p class="p">Correct decision</p>
</td>
</tr>
<tr class="row"><td class="entry" valign="top" width="30.00615195324515%"><p class="p">Fail to reject the null</p>
</td>
<td class="entry" valign="top" width="35.00461396493387%"><p class="p">Correct decision</p>
</td>
<td class="entry" valign="top" width="34.98923408182098%"><p class="p">Type II error</p>
</td>
</tr>
</tbody>
</table>
</div>
<p class="p">According to these definitions, a type I error can only occur if the null hypothesis is true. If the 95% confidence level is chosen, and if the null hypothesis is true, then there is a 95% chance that the null will not be rejected and a 5% probability that the null will be rejected. Unfortunately, we can never know for sure if the null hypothesis is true. </p>
</div>
</div>

</body>
</html>